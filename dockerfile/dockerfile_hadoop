FROM ubuntu:18.04

ENV APP=/app
ENV SHARED_WORKSPACE=/opt/workspace

ENV HADOOP_HOME=/usr/bin/hadoop-3.2.3
ENV HDFS_NAMENODE_USER=hdfs
ENV HDFS_DATANODE_USER=hdfs
ENV HDFS_SECONDARYNAMENODE_USER=hdfs
ENV YARN_RESOURCEMANAGER_USER=hdfs
ENV YARN_NODEMANAGER_USER=hdfs

RUN apt-get update -y && apt-get install vim -y && apt-get install wget -y && apt-get install curl -y && apt-get install ssh -y && apt-get install openjdk-8-jdk -y && apt-get install sudo -y
RUN useradd -m hdfs && echo "hdfs:supergroup" | chpasswd && adduser hdfs sudo && echo "hdfs     ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers && cd /usr/bin/ && sudo ln -s python3 python
RUN wget -q https://downloads.apache.org/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz && tar zxvf hadoop-3.2.3.tar.gz && mv hadoop-3.2.3 $HADOOP_HOME && rm hadoop-3.2.3.tar.gz
RUN chown hdfs -R $HADOOP_HOME

USER hdfs
WORKDIR /home/hdfs
COPY ./conf/ssh_config /etc/ssh/ssh_config
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && chmod 0600 ~/.ssh/authorized_keys
RUN echo "export JAVA_HOME=/usr" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh
COPY ./conf/core-site.xml $HADOOP_HOME/etc/hadoop/
COPY ./conf/hdfs-site.xml $HADOOP_HOME/etc/hadoop/
COPY ./conf/yarn-site.xml $HADOOP_HOME/etc/hadoop/

# Prepare dirs
RUN mkdir -p /tmp/logs/ && chmod a+w /tmp/logs/ && mkdir /app && chmod a+rwx /app && mkdir /data && chmod a+rwx /data
RUN mkdir -p ${SHARED_WORKSPACE}


ADD ./scripts/runHadoop.sh run.sh
ENV PATH $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
EXPOSE 50070 50075 50010 50020 50090 8020 9000 9864 9870 10020 19888 8088 8030 8031 8032 8033 8040 8042 22

ENTRYPOINT ["sh", "run.sh"]
